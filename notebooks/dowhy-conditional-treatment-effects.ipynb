{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Average Treatment Effects (CATE) with DoWhy and EconML\n",
    "\n",
    "This is an experimental feature where we use [EconML](https://github.com/microsoft/econml) methods from DoWhy. Using EconML allows CATE estimation using different methods. \n",
    "\n",
    "All four steps of causal inference in DoWhy remain the same: model, identify, estimate, and refute. The key difference is that we now call econml methods in the estimation step. There is also a simpler example using linear regression to understand the intuition behind CATE estimators. \n",
    "\n",
    "All datasets are generated using linear structural equations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "import dowhy.datasets\n",
    "import econml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "BETA = 10\n",
    "RATIO = 0.30\n",
    "FEATURES='fs_voted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features():\n",
    "    \"\"\"\n",
    "    this method gets a set of features from our previous feature analysis\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        # 1. Features selected from correlations\n",
    "        \"fs_corr\" : ['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean','symmetry_mean',\n",
    "                         'fractal_dimension_mean', 'texture_se', 'area_se','smoothness_se', 'concavity_se',\n",
    "                         'symmetry_se', 'fractal_dimension_se','smoothness_worst', 'concavity_worst', \n",
    "                         'symmetry_worst', 'fractal_dimension_worst'],\n",
    "\n",
    "        # 2. Univariate feature selection SelectKBest, chi2\n",
    "        \"fs_chi2\" : ['texture_mean', 'area_mean', 'concavity_mean', 'symmetry_mean', 'area_se', \n",
    "                         'concavity_se', 'smoothness_worst', 'concavity_worst', 'symmetry_worst', \n",
    "                         'fractal_dimension_worst'],\n",
    "\n",
    "        # 3. Recursive feature elimination (RFE) with random forest\n",
    "        \"fs_rfe\" : ['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean', 'area_se', \n",
    "                  'smoothness_se', 'concavity_se', 'smoothness_worst', 'concavity_worst', 'symmetry_worst'],\n",
    "\n",
    "        # 4. Recursive feature elimination with cross validation(RFECV) with random forest\n",
    "        \"fs_rfecv\" : ['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean','fractal_dimension_mean'\n",
    "                    , 'area_se', 'concavity_se', 'concavity_worst', 'symmetry_worst'],\n",
    "\n",
    "        # 5. Tree based feature selection with random forest classification\n",
    "        \"fs_rf\" : ['texture_mean', 'area_mean', 'concavity_mean', 'area_se', 'concavity_se', \n",
    "                 'fractal_dimension_se', 'smoothness_worst','concavity_worst', 'symmetry_worst', \n",
    "                 'fractal_dimension_worst'],\n",
    "\n",
    "        # 6. ExtraTree based feature selection \n",
    "        \"fs_extraTree\" : ['texture_mean', 'area_mean', 'concavity_mean', 'fractal_dimension_mean', 'area_se', \n",
    "                        'concavity_se','smoothness_worst', 'concavity_worst', \n",
    "                        'symmetry_worst','fractal_dimension_worst'],\n",
    "\n",
    "        # 7. L1 feature selection (LinearSVC)\n",
    "        \"fs_l1\" : ['texture_mean', 'area_mean', 'area_se'],\n",
    "\n",
    "        # 8. Vote based feature selection\n",
    "        \"fs_voted\" : ['texture_mean',  'area_mean',  'smoothness_mean',  'concavity_mean',  \n",
    "                         'fractal_dimension_mean',  'area_se',  'concavity_se',  'smoothness_worst',  \n",
    "                         'concavity_worst',  'symmetry_worst',  'fractal_dimension_worst'],\n",
    "\n",
    "        \n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename,ratio):\n",
    "    \"\"\"\n",
    "    returns complete dataset as split dataset\n",
    "    \n",
    "    args:\n",
    "        model_full (pd.DataFrame): a pandas dataframe containing complete dataset\n",
    "    \n",
    "    returns:\n",
    "        split dataset in form of train and test datasets\n",
    "    \"\"\"\n",
    "    global feature_names, response_name, n_features, model_full  \n",
    "    model_full = pd.read_csv(filename)\n",
    "     \n",
    "    # we change the class values (at the column number 2) from B to 0 and from M to 1\n",
    "    model_full.iloc[:,1].replace('B', 0,inplace=True)\n",
    "    model_full.iloc[:,1].replace('M', 1,inplace=True)\n",
    "    response_name = ['diagnosis']\n",
    "    drop_list = ['Unnamed: 32','id','diagnosis']\n",
    "    model_full_x= model_full.drop(drop_list,axis = 1)\n",
    "    X = model_full_x\n",
    "    y = model_full.diagnosis\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size = ratio,\n",
    "                                                        random_state = 12345)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data('../data/data.csv',RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------columns------\n",
      "Index(['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean',\n",
      "       'fractal_dimension_mean', 'area_se', 'concavity_se', 'smoothness_worst',\n",
      "       'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst'],\n",
      "      dtype='object')\n",
      "--------------------\n",
      "Size of data:\n",
      "The train data has 398 rows and 11 columns\n",
      "----------------------------\n",
      "The test data has 171 rows and 11 columns\n"
     ]
    }
   ],
   "source": [
    "# Voted\n",
    "X_train = x_train[features[FEATURES]]\n",
    "X_test = x_test[features[FEATURES]]\n",
    "\n",
    "# scaling data\n",
    "cols = X_train.columns\n",
    "print('------columns------')\n",
    "print(cols)\n",
    "print('--------------------')\n",
    "sc = StandardScaler()\n",
    "X_train = pd.DataFrame(sc.fit_transform(X_train), columns=cols)\n",
    "X_test = pd.DataFrame(sc.transform(X_test), columns=cols)\n",
    "\n",
    "print('Size of data:')\n",
    "print ('The train data has {0} rows and {1} columns'.format(X_train.shape[0],X_train.shape[1]))\n",
    "print ('----------------------------')\n",
    "print ('The test data has {0} rows and {1} columns'.format(X_test.shape[0],X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df':      texture_mean  area_mean  smoothness_mean  concavity_mean  \\\n",
       " 0        2.625069  -0.289626        -0.271052       -0.761525   \n",
       " 1       -0.158958  -1.233259         0.758922       -1.110366   \n",
       " 2        1.182520  -0.713137         1.875013       -0.551650   \n",
       " 3       -1.969035  -0.705555        -0.717488       -0.941205   \n",
       " 4       -0.011947  -0.161281        -1.134989       -0.739745   \n",
       " ..            ...        ...              ...             ...   \n",
       " 393     -0.057888  -0.703870        -0.752624       -0.431616   \n",
       " 394      1.338720   1.493166         0.352444        2.038985   \n",
       " 395     -0.204899  -0.481161        -0.858033       -1.087349   \n",
       " 396     -0.662013  -0.515705        -0.085725        0.800281   \n",
       " 397     -1.201820  -0.320238         0.738253       -0.394368   \n",
       " \n",
       "      fractal_dimension_mean   area_se  concavity_se  smoothness_worst  \\\n",
       " 0                 -0.386679 -0.253974     -0.993028         -0.660927   \n",
       " 1                  1.548578 -0.477184     -1.335150          0.098381   \n",
       " 2                  1.614863 -0.414155     -0.531145          1.925059   \n",
       " 3                 -0.265636 -0.645233     -0.902755         -1.008040   \n",
       " 4                 -0.994779 -0.264412     -0.675564         -1.129529   \n",
       " ..                      ...       ...           ...               ...   \n",
       " 393               -0.024989 -0.427202      0.581961         -0.326832   \n",
       " 394               -0.246903  0.436531      0.714741         -0.214020   \n",
       " 395               -0.618679 -0.384046     -1.254965         -1.242340   \n",
       " 396                1.499584 -0.228682      3.467328         -0.647911   \n",
       " 397                0.511061 -0.576946     -0.760060          0.297970   \n",
       " \n",
       "      concavity_worst  symmetry_worst  fractal_dimension_worst  \n",
       " 0          -0.987813       -1.162857                -0.414331  \n",
       " 1          -1.334286        0.474049                -0.155605  \n",
       " 2          -0.644110       -0.112538                 0.932096  \n",
       " 3          -0.922457        0.903175                -0.555720  \n",
       " 4          -0.506570       -0.886002                -0.456571  \n",
       " ..               ...             ...                      ...  \n",
       " 393        -0.067727        0.288903                -0.057043  \n",
       " 394         1.472428        0.756096                 0.049145  \n",
       " 395        -1.306684       -0.628181                -1.144159  \n",
       " 396         1.088008        0.628050                 1.149167  \n",
       " 397        -0.419989        0.621129                 0.554276  \n",
       " \n",
       " [398 rows x 11 columns],\n",
       " 'treatment_name': ['v0'],\n",
       " 'outcome_name': 'y',\n",
       " 'common_causes_names': ['W0', 'W1', 'W2', 'W3'],\n",
       " 'instrument_names': ['Z0', 'Z1'],\n",
       " 'effect_modifier_names': ['X0', 'X1'],\n",
       " 'frontdoor_variables_names': [],\n",
       " 'dot_graph': 'digraph {v0->y;W0-> v0; W1-> v0; W2-> v0; W3-> v0;Z0-> v0; Z1-> v0;W0-> y; W1-> y; W2-> y; W3-> y;X0-> y; X1-> y;}',\n",
       " 'gml_graph': 'graph[directed 1node[ id \"y\" label \"y\"]node[ id \"W0\" label \"W0\"] node[ id \"W1\" label \"W1\"] node[ id \"W2\" label \"W2\"] node[ id \"W3\" label \"W3\"]node[ id \"Z0\" label \"Z0\"] node[ id \"Z1\" label \"Z1\"]node[ id \"v0\" label \"v0\"]edge[source \"v0\" target \"y\"]edge[ source \"W0\" target \"v0\"] edge[ source \"W1\" target \"v0\"] edge[ source \"W2\" target \"v0\"] edge[ source \"W3\" target \"v0\"]edge[ source \"Z0\" target \"v0\"] edge[ source \"Z1\" target \"v0\"]edge[ source \"W0\" target \"y\"] edge[ source \"W1\" target \"y\"] edge[ source \"W2\" target \"y\"] edge[ source \"W3\" target \"y\"]node[ id \"X0\" label \"X0\"] edge[ source \"X0\" target \"y\"] node[ id \"X1\" label \"X1\"] edge[ source \"X1\" target \"y\"]]',\n",
       " 'ate': 9.087590563031258}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'df': X_train,\n",
    " 'treatment_name': ['v0'],\n",
    " 'outcome_name': 'y',\n",
    " 'common_causes_names': ['W0', 'W1', 'W2', 'W3'],\n",
    " 'instrument_names': ['Z0', 'Z1'],\n",
    " 'effect_modifier_names': ['X0', 'X1'],\n",
    " 'frontdoor_variables_names': [],\n",
    " 'dot_graph': 'digraph {v0->y;W0-> v0; W1-> v0; W2-> v0; W3-> v0;Z0-> v0; Z1-> v0;W0-> y; W1-> y; W2-> y; W3-> y;X0-> y; X1-> y;}',\n",
    " 'gml_graph': 'graph[directed 1node[ id \"y\" label \"y\"]node[ id \"W0\" label \"W0\"] node[ id \"W1\" label \"W1\"] node[ id \"W2\" label \"W2\"] node[ id \"W3\" label \"W3\"]node[ id \"Z0\" label \"Z0\"] node[ id \"Z1\" label \"Z1\"]node[ id \"v0\" label \"v0\"]edge[source \"v0\" target \"y\"]edge[ source \"W0\" target \"v0\"] edge[ source \"W1\" target \"v0\"] edge[ source \"W2\" target \"v0\"] edge[ source \"W3\" target \"v0\"]edge[ source \"Z0\" target \"v0\"] edge[ source \"Z1\" target \"v0\"]edge[ source \"W0\" target \"y\"] edge[ source \"W1\" target \"y\"] edge[ source \"W2\" target \"y\"] edge[ source \"W3\" target \"y\"]node[ id \"X0\" label \"X0\"] edge[ source \"X0\" target \"y\"] node[ id \"X1\" label \"X1\"] edge[ source \"X1\" target \"y\"]]',\n",
    " 'ate': 9.087590563031258}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         X0        X1   Z0        Z1        W0        W1 W2 W3         v0  \\\n",
      "0  0.138575  0.614958  0.0  0.726732 -0.098521 -0.192259  2  2  19.901447   \n",
      "1  0.210449 -1.477939  1.0  0.138111 -0.229641 -0.402531  3  3  28.731821   \n",
      "2 -0.313951  0.442307  1.0  0.165686 -1.889041 -0.565826  3  2  21.837336   \n",
      "3 -0.011968  0.245486  1.0  0.408266 -1.808235 -1.987021  0  2  11.227033   \n",
      "4 -1.341650 -0.072369  1.0  0.663580 -1.247742  1.417250  2  3  31.658324   \n",
      "\n",
      "            y  \n",
      "0  246.449106  \n",
      "1  137.839716  \n",
      "2  246.773958  \n",
      "3  110.521467  \n",
      "4  301.583673  \n",
      "True causal estimate is 9.087590563031258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'df':             X0        X1   Z0        Z1        W0        W1 W2 W3         v0  \\\n",
       " 0     0.138575  0.614958  0.0  0.726732 -0.098521 -0.192259  2  2  19.901447   \n",
       " 1     0.210449 -1.477939  1.0  0.138111 -0.229641 -0.402531  3  3  28.731821   \n",
       " 2    -0.313951  0.442307  1.0  0.165686 -1.889041 -0.565826  3  2  21.837336   \n",
       " 3    -0.011968  0.245486  1.0  0.408266 -1.808235 -1.987021  0  2  11.227033   \n",
       " 4    -1.341650 -0.072369  1.0  0.663580 -1.247742  1.417250  2  3  31.658324   \n",
       " ...        ...       ...  ...       ...       ...       ... .. ..        ...   \n",
       " 9995  2.482869 -1.183115  1.0  0.190984 -1.671139 -2.592806  1  3  12.266929   \n",
       " 9996  1.176488  0.489390  1.0  0.267129 -0.186638  0.863685  2  2  27.721743   \n",
       " 9997  0.600202  0.923828  1.0  0.948656 -0.393011  0.274843  3  1  35.812123   \n",
       " 9998  1.033206 -0.918504  1.0  0.790974  0.130530 -0.249076  3  2  35.857392   \n",
       " 9999  1.538215  0.261788  1.0  0.083077 -1.583413  0.842975  2  0  19.579996   \n",
       " \n",
       "                y  \n",
       " 0     246.449106  \n",
       " 1     137.839716  \n",
       " 2     246.773958  \n",
       " 3     110.521467  \n",
       " 4     301.583673  \n",
       " ...          ...  \n",
       " 9995   65.681719  \n",
       " 9996  338.497358  \n",
       " 9997  485.588257  \n",
       " 9998  251.677969  \n",
       " 9999  218.656215  \n",
       " \n",
       " [10000 rows x 10 columns],\n",
       " 'treatment_name': ['v0'],\n",
       " 'outcome_name': 'y',\n",
       " 'common_causes_names': ['W0', 'W1', 'W2', 'W3'],\n",
       " 'instrument_names': ['Z0', 'Z1'],\n",
       " 'effect_modifier_names': ['X0', 'X1'],\n",
       " 'frontdoor_variables_names': [],\n",
       " 'dot_graph': 'digraph {v0->y;W0-> v0; W1-> v0; W2-> v0; W3-> v0;Z0-> v0; Z1-> v0;W0-> y; W1-> y; W2-> y; W3-> y;X0-> y; X1-> y;}',\n",
       " 'gml_graph': 'graph[directed 1node[ id \"y\" label \"y\"]node[ id \"W0\" label \"W0\"] node[ id \"W1\" label \"W1\"] node[ id \"W2\" label \"W2\"] node[ id \"W3\" label \"W3\"]node[ id \"Z0\" label \"Z0\"] node[ id \"Z1\" label \"Z1\"]node[ id \"v0\" label \"v0\"]edge[source \"v0\" target \"y\"]edge[ source \"W0\" target \"v0\"] edge[ source \"W1\" target \"v0\"] edge[ source \"W2\" target \"v0\"] edge[ source \"W3\" target \"v0\"]edge[ source \"Z0\" target \"v0\"] edge[ source \"Z1\" target \"v0\"]edge[ source \"W0\" target \"y\"] edge[ source \"W1\" target \"y\"] edge[ source \"W2\" target \"y\"] edge[ source \"W3\" target \"y\"]node[ id \"X0\" label \"X0\"] edge[ source \"X0\" target \"y\"] node[ id \"X1\" label \"X1\"] edge[ source \"X1\" target \"y\"]]',\n",
       " 'ate': 9.087590563031258}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dowhy.datasets.linear_dataset(BETA, num_common_causes=4, num_samples=10000,\n",
    "                                    num_instruments=2, num_effect_modifiers=2,\n",
    "                                     num_treatments=1,\n",
    "                                    treatment_is_binary=False,\n",
    "                                    num_discrete_common_causes=2,\n",
    "                                    num_discrete_effect_modifiers=0,\n",
    "                                    one_hot_encode=False)\n",
    "df=data['df']\n",
    "print(df.head())\n",
    "print(\"True causal estimate is\", data[\"ate\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CausalModel(data=data[\"df\"], \n",
    "                    treatment=data[\"treatment_name\"], outcome=data[\"outcome_name\"], \n",
    "                    graph=data[\"gml_graph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.view_model()\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=\"causal_model.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "identified_estimand= model.identify_effect(proceed_when_unidentifiable=True)\n",
    "print(identified_estimand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model \n",
    "First, let us build some intuition using a linear model for estimating CATE. The effect modifiers (that lead to a heterogeneous treatment effect) can be modeled as interaction terms with the treatment. Thus, their value modulates the effect of treatment. \n",
    "\n",
    "Below the estimated effect of changing treatment from 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_estimate = model.estimate_effect(identified_estimand, \n",
    "                                        method_name=\"backdoor.linear_regression\",\n",
    "                                       control_value=0,\n",
    "                                       treatment_value=1)\n",
    "print(linear_estimate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EconML methods\n",
    "We now move to the more advanced methods from the EconML package for estimating CATE.\n",
    "\n",
    "First, let us look at the double machine learning estimator. Method_name corresponds to the fully qualified name of the class that we want to use. For double ML, it is \"econml.dml.DML\". \n",
    "\n",
    "Target units defines the units over which the causal estimate is to be computed. This can be a lambda function filter on the original dataframe, a new Pandas dataframe, or a string corresponding to the three main kinds of target units (\"ate\", \"att\" and \"atc\"). Below we show an example of a lambda function. \n",
    "\n",
    "Method_params are passed directly to EconML. For details on allowed parameters, refer to the EconML documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "dml_estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.econml.dml.DML\",\n",
    "                                     control_value = 0,\n",
    "                                     treatment_value = 1,\n",
    "                                 target_units = lambda df: df[\"X0\"]>1,  # condition used for CATE\n",
    "                                 confidence_intervals=False,\n",
    "                                method_params={\"init_params\":{'model_y':GradientBoostingRegressor(),\n",
    "                                                              'model_t': GradientBoostingRegressor(),\n",
    "                                                              \"model_final\":LassoCV(fit_intercept=False), \n",
    "                                                              'featurizer':PolynomialFeatures(degree=1, include_bias=False)},\n",
    "                                               \"fit_params\":{}})\n",
    "print(dml_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True causal estimate is\", data[\"ate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.econml.dml.DML\",\n",
    "                                     control_value = 0,\n",
    "                                     treatment_value = 1,\n",
    "                                 target_units = 1,  # condition used for CATE\n",
    "                                 confidence_intervals=False,\n",
    "                                method_params={\"init_params\":{'model_y':GradientBoostingRegressor(),\n",
    "                                                              'model_t': GradientBoostingRegressor(),\n",
    "                                                              \"model_final\":LassoCV(fit_intercept=False), \n",
    "                                                              'featurizer':PolynomialFeatures(degree=1, include_bias=True)},\n",
    "                                               \"fit_params\":{}})\n",
    "print(dml_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATE Object and Confidence Intervals\n",
    "EconML provides its own methods to compute confidence intervals. Using BootstrapInference in the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from econml.inference import BootstrapInference\n",
    "dml_estimate = model.estimate_effect(identified_estimand, \n",
    "                                     method_name=\"backdoor.econml.dml.DML\",\n",
    "                                     target_units = \"ate\",\n",
    "                                     confidence_intervals=True,\n",
    "                                     method_params={\"init_params\":{'model_y':GradientBoostingRegressor(),\n",
    "                                                              'model_t': GradientBoostingRegressor(),\n",
    "                                                              \"model_final\": LassoCV(fit_intercept=False), \n",
    "                                                              'featurizer':PolynomialFeatures(degree=1, include_bias=True)},\n",
    "                                               \"fit_params\":{\n",
    "                                                               'inference': BootstrapInference(n_bootstrap_samples=100, n_jobs=-1),\n",
    "                                                            }\n",
    "                                              })\n",
    "print(dml_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can provide a new inputs as target units and estimate CATE on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols= data['effect_modifier_names'] # only need effect modifiers' values\n",
    "test_arr = [np.random.uniform(0,1, 10) for _ in range(len(test_cols))] # all variables are sampled uniformly, sample of 10\n",
    "test_df = pd.DataFrame(np.array(test_arr).transpose(), columns=test_cols)\n",
    "dml_estimate = model.estimate_effect(identified_estimand, \n",
    "                                     method_name=\"backdoor.econml.dml.DML\",\n",
    "                                     target_units = test_df,\n",
    "                                     confidence_intervals=False,\n",
    "                                     method_params={\"init_params\":{'model_y':GradientBoostingRegressor(),\n",
    "                                                              'model_t': GradientBoostingRegressor(),\n",
    "                                                              \"model_final\":LassoCV(), \n",
    "                                                              'featurizer':PolynomialFeatures(degree=1, include_bias=True)},\n",
    "                                               \"fit_params\":{}\n",
    "                                              })\n",
    "print(dml_estimate.cate_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can also retrieve the raw EconML estimator object for any further operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dml_estimate._estimator_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works with any EconML method\n",
    "In addition to double machine learning, below we example analyses using orthogonal forests, DRLearner (bug to fix), and neural network-based instrumental variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary treatment, Binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binary = dowhy.datasets.linear_dataset(BETA, num_common_causes=4, num_samples=10000,\n",
    "                                    num_instruments=2, num_effect_modifiers=2,\n",
    "                                    treatment_is_binary=True, outcome_is_binary=True)\n",
    "# convert boolean values to {0,1} numeric\n",
    "data_binary['df'].v0 = data_binary['df'].v0.astype(int)\n",
    "data_binary['df'].y = data_binary['df'].y.astype(int)\n",
    "print(data_binary['df'])\n",
    "\n",
    "model_binary = CausalModel(data=data_binary[\"df\"], \n",
    "                    treatment=data_binary[\"treatment_name\"], outcome=data_binary[\"outcome_name\"], \n",
    "                    graph=data_binary[\"gml_graph\"])\n",
    "identified_estimand_binary = model_binary.identify_effect(proceed_when_unidentifiable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using DRLearner estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "#todo needs binary y\n",
    "drlearner_estimate = model_binary.estimate_effect(identified_estimand_binary, \n",
    "                                method_name=\"backdoor.econml.drlearner.LinearDRLearner\",\n",
    "                                confidence_intervals=False,\n",
    "                                method_params={\"init_params\":{\n",
    "                                                    'model_propensity': LogisticRegressionCV(cv=3, solver='lbfgs', multi_class='auto')\n",
    "                                                    },\n",
    "                                               \"fit_params\":{}\n",
    "                                              })\n",
    "print(drlearner_estimate)\n",
    "print(\"True causal estimate is\", data_binary[\"ate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumental Variable Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from econml.deepiv import DeepIVEstimator\n",
    "dims_zx = len(model.get_instruments())+len(model.get_effect_modifiers())\n",
    "dims_tx = len(model._treatment)+len(model.get_effect_modifiers())\n",
    "treatment_model = keras.Sequential([keras.layers.Dense(128, activation='relu', input_shape=(dims_zx,)), # sum of dims of Z and X \n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(64, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(32, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17)])                \n",
    "response_model = keras.Sequential([keras.layers.Dense(128, activation='relu', input_shape=(dims_tx,)), # sum of dims of T and X\n",
    "                                    keras.layers.Dropout(0.17), \n",
    "                                    keras.layers.Dense(64, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(32, activation='relu'),\n",
    "                                    keras.layers.Dropout(0.17),\n",
    "                                    keras.layers.Dense(1)])\n",
    "\n",
    "deepiv_estimate = model.estimate_effect(identified_estimand, \n",
    "                                        method_name=\"iv.econml.deepiv.DeepIV\",\n",
    "                                        target_units = lambda df: df[\"X0\"]>-1, \n",
    "                                        confidence_intervals=False,\n",
    "                                method_params={\"init_params\":{'n_components': 10, # Number of gaussians in the mixture density networks\n",
    "                                                              'm': lambda z, x: treatment_model(keras.layers.concatenate([z, x])), # Treatment model,\n",
    "                                                              \"h\": lambda t, x: response_model(keras.layers.concatenate([t, x])), # Response model\n",
    "                                                              'n_samples': 1, # Number of samples used to estimate the response\n",
    "                                                              'first_stage_options': {'epochs':25},\n",
    "                                                              'second_stage_options': {'epochs':25}\n",
    "                                                             },\n",
    "                                               \"fit_params\":{}})\n",
    "print(deepiv_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metalearners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_experiment = dowhy.datasets.linear_dataset(BETA, num_common_causes=5, num_samples=10000,\n",
    "                                    num_instruments=2, num_effect_modifiers=5,\n",
    "                                    treatment_is_binary=True, outcome_is_binary=False)\n",
    "# convert boolean values to {0,1} numeric\n",
    "data_experiment['df'].v0 = data_experiment['df'].v0.astype(int)\n",
    "print(data_experiment['df'])\n",
    "model_experiment = CausalModel(data=data_experiment[\"df\"], \n",
    "                    treatment=data_experiment[\"treatment_name\"], outcome=data_experiment[\"outcome_name\"], \n",
    "                    graph=data_experiment[\"gml_graph\"])\n",
    "identified_estimand_experiment = model_experiment.identify_effect(proceed_when_unidentifiable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "metalearner_estimate = model_experiment.estimate_effect(identified_estimand_experiment, \n",
    "                                method_name=\"backdoor.econml.metalearners.TLearner\",\n",
    "                                confidence_intervals=False,\n",
    "                                method_params={\"init_params\":{\n",
    "                                                    'models': RandomForestRegressor()\n",
    "                                                    },\n",
    "                                               \"fit_params\":{}\n",
    "                                              })\n",
    "print(metalearner_estimate)\n",
    "print(\"True causal estimate is\", data_experiment[\"ate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding retraining the estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an estimator is fitted, it can be reused to estimate effect on different data points. In this case, you can pass `fit_estimator=False` to `estimate_effect`. This works for any EconML estimator. We show an example for the T-learner below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For metalearners, need to provide all the features (except treatmeant and outcome)\n",
    "metalearner_estimate = model_experiment.estimate_effect(identified_estimand_experiment, \n",
    "                                method_name=\"backdoor.econml.metalearners.TLearner\",\n",
    "                                confidence_intervals=False,\n",
    "                                fit_estimator=False,\n",
    "                                target_units=data_experiment[\"df\"].drop([\"v0\",\"y\", \"Z0\", \"Z1\"], axis=1)[9995:],                        \n",
    "                                method_params={})\n",
    "print(metalearner_estimate)\n",
    "print(\"True causal estimate is\", data_experiment[\"ate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refuting the estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a random common cause variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_random=model.refute_estimate(identified_estimand, dml_estimate, method_name=\"random_common_cause\")\n",
    "print(res_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an unobserved common cause variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_unobserved=model.refute_estimate(identified_estimand, dml_estimate, method_name=\"add_unobserved_common_cause\",\n",
    "                                     confounders_effect_on_treatment=\"linear\", confounders_effect_on_outcome=\"linear\",\n",
    "                                    effect_strength_on_treatment=0.01, effect_strength_on_outcome=0.02)\n",
    "print(res_unobserved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing treatment with a random (placebo) variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_placebo=model.refute_estimate(identified_estimand, dml_estimate,\n",
    "        method_name=\"placebo_treatment_refuter\", placebo_type=\"permute\",\n",
    "        num_simulations=10 # at least 100 is good, setting to 10 for speed \n",
    "        ) \n",
    "print(res_placebo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing a random subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_subset=model.refute_estimate(identified_estimand, dml_estimate,\n",
    "        method_name=\"data_subset_refuter\", subset_fraction=0.8,\n",
    "        num_simulations=10)\n",
    "print(res_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More refutation methods to come, especially specific to the CATE estimators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
